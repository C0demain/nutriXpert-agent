import os
import pandas as pd
import pdfplumber
import logging
from datetime import datetime
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.schema import Document

# --------- CONFIGURA√á√ÉO DE LOGGING ---------
LOG_DIR = "logs"
os.makedirs(LOG_DIR, exist_ok=True)
LOG_FILE = os.path.join(LOG_DIR, f"ingest_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(LOG_FILE, encoding="utf-8"),
        logging.StreamHandler()
    ],
)

CHROMA_PATH = "chroma_store"
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DOCS_DIR = os.path.join(BASE_DIR, "..", "..", "documents")

# --------- Loaders ---------
def load_pdf(path: str) -> str:
    """Extrai texto de um PDF."""
    logging.info(f"üìÑ Lendo PDF: {os.path.basename(path)}")
    with pdfplumber.open(path) as pdf:
        pages = [p.extract_text() for p in pdf.pages if p.extract_text()]
    text = "\n".join(pages)
    logging.info(f"‚úÖ Extra√≠das {len(pages)} p√°ginas ({len(text)} caracteres).")
    return text


def load_xlsx(path: str) -> str:
    """Extrai dados tabulares de um XLSX (TACO)."""
    logging.info(f"üìä Lendo planilha TACO: {os.path.basename(path)}")
    try:
        df = pd.read_excel(path, sheet_name="Taco")
    except Exception as e:
        logging.error(f"Erro ao ler planilha: {e}")
        return ""

    expected_cols = {
        "N√∫mero", "Grupo", "Descri√ß√£o do Alimento", "Umidade(%)", "Energia(kcal)", "Energia(kJ)",
        "Prote√≠na(g)", "Lip√≠deos(g)", "Colesterol(mg)", "Carboidrato(g)", "Fibra Alimentar(g)",
        "Cinzas(g)", "C√°lcio(mg)", "Magn√©sio(mg)", "Mangan√™s(mg)", "F√≥sforo(mg)",
        "Ferro(mg)", "S√≥dio(mg)", "Pot√°ssio(mg)", "Cobre(mg)", "Zinco(mg)",
        "Retinol(mcg)", "RE(mcg)", "RAE(mcg)", "Tiamina(mg)", "Riboflavina(mg)",
        "Piridoxina(mg)", "Niacina(mg)", "VitaminaC(mg)"
    }

    missing = expected_cols - set(df.columns)
    if missing:
        logging.warning(f"‚ö†Ô∏è Colunas esperadas ausentes: {missing}")

    lines = []
    for _, row in df.iterrows():
        alimento = str(row.get("Descri√ß√£o do Alimento", "")).strip()
        if not alimento:
            continue

        comps = [
            f"{col}: {row[col]}"
            for col in df.columns
            if col != "Descri√ß√£o do Alimento" and pd.notna(row[col])
        ]
        lines.append(f"{alimento} -> {', '.join(comps)}")

    logging.info(f"‚úÖ {len(lines)} alimentos extra√≠dos da TACO ({len(df)} linhas na planilha).")
    if len(lines) > 0:
        sample = "\n".join(lines[:3])
        logging.debug(f"Exemplo dos primeiros alimentos:\n{sample}")

    return "\n".join(lines)


def ingest_documents(folder=DOCS_DIR):
    """Carrega todos os PDFs e XLSX da pasta documents."""
    logging.info(f"üìÇ Iniciando ingest√£o da pasta: {os.path.abspath(folder)}")
    docs = []
    if not os.path.exists(folder):
        logging.error(f"‚ùå Pasta n√£o encontrada: {folder}")
        return docs

    for fname in os.listdir(folder):
        path = os.path.join(folder, fname)
        if fname.lower().endswith(".pdf"):
            text = load_pdf(path)
            docs.append({"text": text, "metadata": {"source": fname, "type": "pdf"}})
        elif fname.lower().endswith(".xlsx"):
            text = load_xlsx(path)
            docs.append({"text": text, "metadata": {"source": fname, "type": "taco"}})
        else:
            logging.info(f"‚è≠Ô∏è Ignorado: {fname}")
    logging.info(f"üìö Total de documentos carregados: {len(docs)}")
    return docs

# --------- Embeddings Locais ---------
def get_embeddings():
    """Usa modelo local do HuggingFace para gerar embeddings."""
    logging.info("üß† Carregando modelo de embeddings: sentence-transformers/all-MiniLM-L6-v2")
    return HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# --------- Vetoriza√ß√£o ---------
def build_vectorstore(docs):
    """Divide em chunks, gera embeddings e salva no Chroma."""
    if not docs:
        logging.warning("‚ö†Ô∏è Nenhum documento para vetorizar.")
        return None

    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = []
    total_chars = 0

    for doc in docs:
        splitted = splitter.split_text(doc["text"])
        total_chars += len(doc["text"])
        for chunk in splitted:
            chunks.append(Document(page_content=chunk, metadata=doc["metadata"]))

    logging.info(f"‚úÇÔ∏è Total de chunks criados: {len(chunks)} (a partir de {len(docs)} documentos, {total_chars:,} caracteres).")

    embeddings = get_embeddings()
    vectordb = Chroma.from_documents(
        chunks,
        embedding=embeddings,
        persist_directory=CHROMA_PATH
    )

    vectordb.persist()
    logging.info(f"üíæ Vetoriza√ß√£o conclu√≠da e persistida em: {CHROMA_PATH}")
    logging.info(f"‚úÖ Base vetorial cont√©m {vectordb._collection.count()} embeddings.")

    return vectordb

